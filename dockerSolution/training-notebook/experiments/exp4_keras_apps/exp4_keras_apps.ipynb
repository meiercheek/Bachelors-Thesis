{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2f1754d",
   "metadata": {
    "id": "7v55rWlQehzL",
    "papermill": {
     "duration": 0.030147,
     "end_time": "2021-12-02T14:04:00.161222",
     "exception": false,
     "start_time": "2021-12-02T14:04:00.131075",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Experiment 4 (section 6.4)\n",
    "Finding out which keras apps model suits our dataset the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceafc7b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T14:04:00.210702Z",
     "iopub.status.busy": "2021-12-02T14:04:00.209517Z",
     "iopub.status.idle": "2021-12-02T14:04:05.862962Z",
     "shell.execute_reply": "2021-12-02T14:04:05.861845Z",
     "shell.execute_reply.started": "2021-12-02T13:15:13.914078Z"
    },
    "id": "dn-6c02VmqiN",
    "outputId": "80a3d727-a0b6-4776-af42-40b8f9bbe64e",
    "papermill": {
     "duration": 5.685054,
     "end_time": "2021-12-02T14:04:05.863175",
     "exception": false,
     "start_time": "2021-12-02T14:04:00.178121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from shutil import copyfile\n",
    "import numpy as np\n",
    "import matplotlib.image  as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "now = str(datetime.now())\n",
    "%load_ext tensorboard\n",
    "import inspect\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db44603",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ffb89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "zippath='../../datasets/dataset.zip'\n",
    "with zipfile.ZipFile(zippath, 'r') as zip_ref:\n",
    "    zip_ref.extractall('raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62b8b10b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T14:04:05.984179Z",
     "iopub.status.busy": "2021-12-02T14:04:05.983145Z",
     "iopub.status.idle": "2021-12-02T14:04:05.988680Z",
     "shell.execute_reply": "2021-12-02T14:04:05.988204Z",
     "shell.execute_reply.started": "2021-12-02T13:15:27.872782Z"
    },
    "id": "v_om9G9UXID6",
    "papermill": {
     "duration": 0.024144,
     "end_time": "2021-12-02T14:04:05.988795",
     "exception": false,
     "start_time": "2021-12-02T14:04:05.964651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List all available models\n",
    "model_dictionary = {m[0]:m[1] for m in inspect.getmembers(tf.keras.applications, inspect.isfunction)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75311c30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T14:04:06.028432Z",
     "iopub.status.busy": "2021-12-02T14:04:06.027494Z",
     "iopub.status.idle": "2021-12-02T14:04:06.031571Z",
     "shell.execute_reply": "2021-12-02T14:04:06.032157Z",
     "shell.execute_reply.started": "2021-12-02T13:30:26.202435Z"
    },
    "id": "MZG1ZJMPXP7D",
    "outputId": "25588d26-075e-49fe-9f96-dd00b1c9d694",
    "papermill": {
     "duration": 0.02757,
     "end_time": "2021-12-02T14:04:06.032337",
     "exception": false,
     "start_time": "2021-12-02T14:04:06.004767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DenseNet121': <function keras.applications.densenet.DenseNet121(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)>,\n",
       " 'DenseNet169': <function keras.applications.densenet.DenseNet169(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)>,\n",
       " 'DenseNet201': <function keras.applications.densenet.DenseNet201(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)>,\n",
       " 'EfficientNetB0': <function keras.applications.efficientnet.EfficientNetB0(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000, classifier_activation='softmax', **kwargs)>,\n",
       " 'EfficientNetB1': <function keras.applications.efficientnet.EfficientNetB1(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000, classifier_activation='softmax', **kwargs)>,\n",
       " 'EfficientNetB2': <function keras.applications.efficientnet.EfficientNetB2(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000, classifier_activation='softmax', **kwargs)>,\n",
       " 'EfficientNetB3': <function keras.applications.efficientnet.EfficientNetB3(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000, classifier_activation='softmax', **kwargs)>,\n",
       " 'EfficientNetB4': <function keras.applications.efficientnet.EfficientNetB4(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000, classifier_activation='softmax', **kwargs)>,\n",
       " 'EfficientNetB5': <function keras.applications.efficientnet.EfficientNetB5(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000, classifier_activation='softmax', **kwargs)>,\n",
       " 'EfficientNetB6': <function keras.applications.efficientnet.EfficientNetB6(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000, classifier_activation='softmax', **kwargs)>,\n",
       " 'EfficientNetB7': <function keras.applications.efficientnet.EfficientNetB7(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000, classifier_activation='softmax', **kwargs)>,\n",
       " 'InceptionResNetV2': <function keras.applications.inception_resnet_v2.InceptionResNetV2(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000, classifier_activation='softmax', **kwargs)>,\n",
       " 'InceptionV3': <function keras.applications.inception_v3.InceptionV3(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000, classifier_activation='softmax')>,\n",
       " 'MobileNet': <function keras.applications.mobilenet.MobileNet(input_shape=None, alpha=1.0, depth_multiplier=1, dropout=0.001, include_top=True, weights='imagenet', input_tensor=None, pooling=None, classes=1000, classifier_activation='softmax', **kwargs)>,\n",
       " 'MobileNetV2': <function keras.applications.mobilenet_v2.MobileNetV2(input_shape=None, alpha=1.0, include_top=True, weights='imagenet', input_tensor=None, pooling=None, classes=1000, classifier_activation='softmax', **kwargs)>,\n",
       " 'MobileNetV3Large': <function keras.applications.mobilenet_v3.MobileNetV3Large(input_shape=None, alpha=1.0, minimalistic=False, include_top=True, weights='imagenet', input_tensor=None, classes=1000, pooling=None, dropout_rate=0.2, classifier_activation='softmax', include_preprocessing=True)>,\n",
       " 'MobileNetV3Small': <function keras.applications.mobilenet_v3.MobileNetV3Small(input_shape=None, alpha=1.0, minimalistic=False, include_top=True, weights='imagenet', input_tensor=None, classes=1000, pooling=None, dropout_rate=0.2, classifier_activation='softmax', include_preprocessing=True)>,\n",
       " 'NASNetLarge': <function keras.applications.nasnet.NASNetLarge(input_shape=None, include_top=True, weights='imagenet', input_tensor=None, pooling=None, classes=1000)>,\n",
       " 'NASNetMobile': <function keras.applications.nasnet.NASNetMobile(input_shape=None, include_top=True, weights='imagenet', input_tensor=None, pooling=None, classes=1000)>,\n",
       " 'ResNet101': <function keras.applications.resnet.ResNet101(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000, **kwargs)>,\n",
       " 'ResNet101V2': <function keras.applications.resnet_v2.ResNet101V2(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000, classifier_activation='softmax')>,\n",
       " 'ResNet152': <function keras.applications.resnet.ResNet152(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000, **kwargs)>,\n",
       " 'ResNet152V2': <function keras.applications.resnet_v2.ResNet152V2(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000, classifier_activation='softmax')>,\n",
       " 'ResNet50': <function keras.applications.resnet.ResNet50(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000, **kwargs)>,\n",
       " 'ResNet50V2': <function keras.applications.resnet_v2.ResNet50V2(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000, classifier_activation='softmax')>,\n",
       " 'VGG16': <function keras.applications.vgg16.VGG16(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000, classifier_activation='softmax')>,\n",
       " 'VGG19': <function keras.applications.vgg19.VGG19(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000, classifier_activation='softmax')>,\n",
       " 'Xception': <function keras.applications.xception.Xception(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000, classifier_activation='softmax')>}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32825992",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T14:04:06.110161Z",
     "iopub.status.busy": "2021-12-02T14:04:06.109366Z",
     "iopub.status.idle": "2021-12-02T14:04:06.842039Z",
     "shell.execute_reply": "2021-12-02T14:04:06.841169Z",
     "shell.execute_reply.started": "2021-12-02T10:18:09.420678Z"
    },
    "id": "MzNqqZNWxoF5",
    "papermill": {
     "duration": 0.753668,
     "end_time": "2021-12-02T14:04:06.842215",
     "exception": false,
     "start_time": "2021-12-02T14:04:06.088547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce93d971",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T14:04:06.883951Z",
     "iopub.status.busy": "2021-12-02T14:04:06.882944Z",
     "iopub.status.idle": "2021-12-02T14:04:50.386975Z",
     "shell.execute_reply": "2021-12-02T14:04:50.388070Z",
     "shell.execute_reply.started": "2021-12-02T13:16:45.987960Z"
    },
    "id": "_j50SL9zv2Rv",
    "outputId": "02dd1702-e594-44f9-cd3a-f32c725e7d43",
    "papermill": {
     "duration": 43.529482,
     "end_time": "2021-12-02T14:04:50.388346",
     "exception": false,
     "start_time": "2021-12-02T14:04:06.858864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../input/hribike3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:43<00:00,  1.81s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from shutil import copyfile\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "def split_data(SOURCE, TRAINING, VALID, TESTING, splitsizetrain, splitsizevalid, splitsizetest):\n",
    "    files = []\n",
    "\n",
    "    os.mkdir(TRAINING)\n",
    "    os.mkdir(VALID)\n",
    "    os.mkdir(TESTING)\n",
    "\n",
    "\n",
    "    for filename in os.listdir(SOURCE):\n",
    "        file = SOURCE + filename\n",
    "        if os.path.getsize(file) > 0:\n",
    "            files.append(filename)\n",
    "        else:\n",
    "            print(filename + \" is zero length, so ignoring.\")\n",
    "\n",
    "    training_length = int(len(files) * splitsizetrain)\n",
    "    valid_length = int(len(files) * 0.8)\n",
    "    #testing_length = int(len(files) - valid_length)\n",
    "    #print(training_length, valid_length, testing_length)\n",
    "    shuffled_set = random.sample(files, len(files))\n",
    "\n",
    "    training_set = shuffled_set[:training_length]\n",
    "\n",
    "    testing_set = shuffled_set[training_length:valid_length]\n",
    "    #print(training_length,valid_length)\n",
    "    valid_set = shuffled_set[valid_length:]\n",
    "\n",
    "\n",
    "    for filename in training_set:\n",
    "        this_file = SOURCE + filename\n",
    "        destination = TRAINING + filename\n",
    "        copyfile(this_file, destination)\n",
    "\n",
    "    for filename in valid_set:\n",
    "        this_file = SOURCE + filename\n",
    "        destination = VALID + filename\n",
    "        copyfile(this_file, destination)\n",
    "\n",
    "    for filename in testing_set:\n",
    "        this_file = SOURCE + filename\n",
    "        destination = TESTING + filename\n",
    "        copyfile(this_file, destination)    \n",
    "\n",
    "p = Path('raw')\n",
    "print(p)\n",
    "folders = [x for x in p.iterdir() if x.is_dir()]\n",
    "split_size_train = .6\n",
    "split_size_valid = .2\n",
    "split_size_test = .2\n",
    "\n",
    "try:\n",
    "  os.mkdir('tmp')\n",
    "except:\n",
    "  print('error')\n",
    "\n",
    "try:  \n",
    "  os.mkdir('tmp/train')\n",
    "  os.mkdir('tmp/valid')\n",
    "  os.mkdir('tmp/test')\n",
    "except:\n",
    "  print('error but here')  \n",
    "\n",
    "for f in tqdm(folders):\n",
    "    z = Path(f)\n",
    "    source = './'+str(z)+'/'\n",
    "\n",
    "    split_data(source, './tmp/train/'+z.parts[1]+'/', './tmp/valid/'+z.parts[1]+'/',\n",
    "                './tmp/test/'+z.parts[1]+'/',split_size_train, split_size_valid, split_size_test)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15f9383f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T14:04:50.451902Z",
     "iopub.status.busy": "2021-12-02T14:04:50.450927Z",
     "iopub.status.idle": "2021-12-02T14:04:51.180948Z",
     "shell.execute_reply": "2021-12-02T14:04:51.180378Z",
     "shell.execute_reply.started": "2021-12-02T13:42:35.691993Z"
    },
    "id": "fQrZfVgz4j2g",
    "outputId": "a407e9b5-dce9-48d0-bf9c-575724dae235",
    "papermill": {
     "duration": 0.766108,
     "end_time": "2021-12-02T14:04:51.181108",
     "exception": false,
     "start_time": "2021-12-02T14:04:50.415000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4338 images belonging to 24 classes.\n",
      "Found 1462 images belonging to 24 classes.\n",
      "Found 1447 images belonging to 24 classes.\n",
      "4338\n",
      "1462\n",
      "1447\n"
     ]
    }
   ],
   "source": [
    "BATCHSIZE = 20\n",
    "TRAINING_DIR = \"./tmp/train/\"\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255.,  #mobilenet requires -1;1 range\n",
    "                                   rotation_range=40,\n",
    "                                    width_shift_range=0.2,\n",
    "                                    height_shift_range=0.2,\n",
    "                                    shear_range=0.2,\n",
    "                                    zoom_range=0.2,\n",
    "                                    horizontal_flip=True,\n",
    "                                    fill_mode='nearest')\n",
    "train_generator = train_datagen.flow_from_directory(TRAINING_DIR,\n",
    "                                                    batch_size=BATCHSIZE,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    target_size=(224, 224))\n",
    "\n",
    "VALIDATION_DIR = \"./tmp/valid/\"\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255.)\n",
    "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR,\n",
    "                                                              batch_size=BATCHSIZE,\n",
    "                                                              class_mode='categorical',\n",
    "                                                              target_size=(224, 224))\n",
    "\n",
    "TESTING_DIR = \"./tmp/test/\"\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.)\n",
    "test_generator = test_datagen.flow_from_directory(TESTING_DIR,\n",
    "                                                              batch_size=BATCHSIZE,\n",
    "                                                              shuffle=False,\n",
    "                                                              class_mode='categorical',\n",
    "                                                              target_size=(224, 224))\n",
    "7247\n",
    "print(len(train_generator.classes))\n",
    "print(len(validation_generator.classes))\n",
    "print(len(test_generator.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8762707f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T14:04:51.245687Z",
     "iopub.status.busy": "2021-12-02T14:04:51.244425Z",
     "iopub.status.idle": "2021-12-02T14:04:51.248131Z",
     "shell.execute_reply": "2021-12-02T14:04:51.248821Z",
     "shell.execute_reply.started": "2021-12-02T13:42:40.075630Z"
    },
    "id": "3q8pJv63yFF2",
    "outputId": "b336d82f-7434-4de5-bac9-b4b1842dbc4d",
    "papermill": {
     "duration": 0.039555,
     "end_time": "2021-12-02T14:04:51.248972",
     "exception": false,
     "start_time": "2021-12-02T14:04:51.209417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Agaricus_arvensis',\n",
       " 'Amanita_muscaria',\n",
       " 'Amanita_pantherina',\n",
       " 'Amanita_phalloides',\n",
       " 'Auricularia_auricula-judae',\n",
       " 'Boletus_reticulatus_edulis',\n",
       " 'Calocera_viscosa',\n",
       " 'Cantharellus_cibarius',\n",
       " 'Chlorociboria_aeruginascens',\n",
       " 'Cortinarius_semisanguineus',\n",
       " 'Cortinarius_violaceus',\n",
       " 'Galerina_marginata',\n",
       " 'Gyromitra_esculenta',\n",
       " 'Hygrocybe_cantharellus',\n",
       " 'Lactarius_deliciosus',\n",
       " 'Leccinum_scabrum',\n",
       " 'Lepista_nuda',\n",
       " 'Macrolepiota_procera',\n",
       " 'Pleurotus_ostreatus',\n",
       " 'Rubroboletus_satanas',\n",
       " 'Russula_cyanoxantha',\n",
       " 'Russula_lepida',\n",
       " 'Sarcoscypha_austriaca',\n",
       " 'Suillus_luteus']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = list(dict.keys(train_generator.class_indices))\n",
    "(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "317f2c20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T14:04:51.309737Z",
     "iopub.status.busy": "2021-12-02T14:04:51.308854Z",
     "iopub.status.idle": "2021-12-02T14:04:52.000053Z",
     "shell.execute_reply": "2021-12-02T14:04:51.999297Z",
     "shell.execute_reply.started": "2021-12-02T13:42:43.796974Z"
    },
    "id": "EMkJnn_vpGM8",
    "outputId": "7cdaf9f6-ee36-429a-cfed-d6f129e1949c",
    "papermill": {
     "duration": 0.723888,
     "end_time": "2021-12-02T14:04:52.000201",
     "exception": false,
     "start_time": "2021-12-02T14:04:51.276313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "weights = compute_class_weight('balanced', classes=np.unique(train_generator.classes), y=train_generator.classes)\n",
    "cw = dict(zip( np.unique(train_generator.classes), weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41f52051",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T14:04:52.063947Z",
     "iopub.status.busy": "2021-12-02T14:04:52.060860Z",
     "iopub.status.idle": "2021-12-02T14:04:52.068824Z",
     "shell.execute_reply": "2021-12-02T14:04:52.068230Z",
     "shell.execute_reply.started": "2021-12-02T13:42:46.699724Z"
    },
    "papermill": {
     "duration": 0.03795,
     "end_time": "2021-12-02T14:04:52.068953",
     "exception": false,
     "start_time": "2021-12-02T14:04:52.031003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1.1737012987012987, 1: 0.9877049180327869, 2: 1.0954545454545455, 3: 0.926923076923077, 4: 1.063235294117647, 5: 0.9665775401069518, 6: 0.94140625, 7: 0.9717741935483871, 8: 0.9931318681318682, 9: 1.069526627218935, 10: 0.9563492063492064, 11: 0.9986187845303868, 12: 0.9665775401069518, 13: 0.926923076923077, 14: 1.0508720930232558, 15: 0.9877049180327869, 16: 0.9513157894736842, 17: 0.9986187845303868, 18: 1.2819148936170213, 19: 1.1157407407407407, 20: 0.844626168224299, 21: 1.1088957055214723, 22: 0.8689903846153846, 23: 0.9563492063492064}\n"
     ]
    }
   ],
   "source": [
    "print(cw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "337b5429",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T14:04:52.130599Z",
     "iopub.status.busy": "2021-12-02T14:04:52.128511Z",
     "iopub.status.idle": "2021-12-02T14:04:52.131308Z",
     "shell.execute_reply": "2021-12-02T14:04:52.131810Z",
     "shell.execute_reply.started": "2021-12-02T13:46:42.606191Z"
    },
    "id": "7UZ9Lh1kd851",
    "papermill": {
     "duration": 0.035492,
     "end_time": "2021-12-02T14:04:52.131951",
     "exception": false,
     "start_time": "2021-12-02T14:04:52.096459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_benchmarks = {'model_name': [], 'num_model_params': [], 'validation_accuracy': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "307e9a3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T14:04:52.193720Z",
     "iopub.status.busy": "2021-12-02T14:04:52.192841Z",
     "iopub.status.idle": "2021-12-02T14:04:52.196368Z",
     "shell.execute_reply": "2021-12-02T14:04:52.196850Z",
     "shell.execute_reply.started": "2021-12-02T13:46:48.457572Z"
    },
    "id": "sFulRyVAfnNk",
    "outputId": "50764439-0fa4-4cc9-e4e2-ea68b980add2",
    "papermill": {
     "duration": 0.036556,
     "end_time": "2021-12-02T14:04:52.196987",
     "exception": false,
     "start_time": "2021-12-02T14:04:52.160431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': [], 'num_model_params': [], 'validation_accuracy': []}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d3558e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T14:04:52.267460Z",
     "iopub.status.busy": "2021-12-02T14:04:52.266399Z",
     "iopub.status.idle": "2021-12-02T14:04:52.269787Z",
     "shell.execute_reply": "2021-12-02T14:04:52.270356Z",
     "shell.execute_reply.started": "2021-12-02T13:46:50.901082Z"
    },
    "papermill": {
     "duration": 0.045362,
     "end_time": "2021-12-02T14:04:52.270503",
     "exception": false,
     "start_time": "2021-12-02T14:04:52.225141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import applications\n",
    "def getGenerators(modelname):\n",
    "    PF = None\n",
    "    TS = (224, 224)\n",
    "    BATCHSIZE = 20\n",
    "    if 'DenseNet' in modelname:\n",
    "        PF = applications.densenet.preprocess_input\n",
    "    elif 'InceptionResNetV2' in modelname:\n",
    "        PF = applications.inception_resnet_v2.preprocess_input\n",
    "    elif 'InceptionV3' in modelname: \n",
    "        PF = applications.inception_v3.preprocess_input\n",
    "    elif 'MobileNet' in modelname:\n",
    "        PF = applications.mobilenet.preprocess_input\n",
    "    elif 'MobileNetV2' in modelname:\n",
    "        PF = applications.mobilenet_v2.preprocess_input\n",
    "    elif 'NASNetLarge' in modelname:\n",
    "        PF = applications.nasnet.preprocess_input\n",
    "        TS = (331, 331)\n",
    "    elif 'NASNetMobile' in modelname:\n",
    "        PF = applications.nasnet.preprocess_input\n",
    "    elif 'ResNet101' in modelname:\n",
    "        PF = applications.resnet50.preprocess_input\n",
    "    elif 'ResNet101V2' in modelname:\n",
    "        PF = applications.resnet_v2.preprocess_input\n",
    "    elif 'ResNet152' in modelname:\n",
    "        PF = applications.resnet50.preprocess_input\n",
    "    elif 'ResNet152V2' in modelname:\n",
    "        PF = applications.resnet_v2.preprocess_input\n",
    "    elif 'ResNet50' in modelname:\n",
    "        PF = applications.resnet50.preprocess_input\n",
    "    elif 'ResNet50V2' in modelname:\n",
    "        PF = applications.resnet_v2.preprocess_input\n",
    "    elif 'VGG16' in modelname:\n",
    "        PF = applications.vgg16.preprocess_input\n",
    "    elif 'VGG19' in modelname:\n",
    "        PF = applications.vgg19.preprocess_input\n",
    "    elif 'Xception' in modelname:\n",
    "        PF = applications.xception.preprocess_input   \n",
    "    \n",
    "    train_datagen = ImageDataGenerator(preprocessing_function=PF, \n",
    "                                   rotation_range=40,\n",
    "                                    width_shift_range=0.2,\n",
    "                                    height_shift_range=0.2,\n",
    "                                    shear_range=0.2,\n",
    "                                    zoom_range=0.2,\n",
    "                                    horizontal_flip=True,\n",
    "                                    fill_mode='nearest')\n",
    "    \n",
    "    train_generator = train_datagen.flow_from_directory(TRAINING_DIR,\n",
    "                                                        batch_size=BATCHSIZE,\n",
    "                                                        class_mode='categorical',\n",
    "                                                        target_size=TS)\n",
    "    \n",
    "    validation_datagen = ImageDataGenerator(preprocessing_function=PF)\n",
    "    \n",
    "    validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR,\n",
    "                                                                  batch_size=BATCHSIZE,\n",
    "                                                                  class_mode='categorical',\n",
    "                                                                  target_size=TS)\n",
    "\n",
    "    return train_generator, validation_generator, TS\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66de44a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T14:04:52.338864Z",
     "iopub.status.busy": "2021-12-02T14:04:52.337925Z",
     "iopub.status.idle": "2021-12-02T17:07:44.598228Z",
     "shell.execute_reply": "2021-12-02T17:07:44.598807Z",
     "shell.execute_reply.started": "2021-12-02T13:50:09.088601Z"
    },
    "id": "5qE1G6JB4fMn",
    "outputId": "e1973cca-edd1-4983-fd3e-624d4d86de60",
    "papermill": {
     "duration": 10972.300788,
     "end_time": "2021-12-02T17:07:44.599017",
     "exception": false,
     "start_time": "2021-12-02T14:04:52.298229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________\n",
      "DenseNet121\n",
      "Found 4338 images belonging to 24 classes.\n",
      "Found 1462 images belonging to 24 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 14:04:53.278388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-02 14:04:53.524687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-02 14:04:53.525814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-02 14:04:53.528234: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-02 14:04:53.529649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-02 14:04:53.530874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-02 14:04:53.532056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-02 14:04:55.862683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-02 14:04:55.863803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-02 14:04:55.864697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-02 14:04:55.865614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "29089792/29084464 [==============================] - 0s 0us/step\n",
      "29097984/29084464 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 14:05:00.978912: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 14:05:09.774375: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217/217 [==============================] - 104s 415ms/step - loss: 2.3662 - accuracy: 0.3628 - val_loss: 1.0836 - val_accuracy: 0.6888\n",
      "Epoch 2/3\n",
      "217/217 [==============================] - 87s 397ms/step - loss: 1.1771 - accuracy: 0.6478 - val_loss: 0.7223 - val_accuracy: 0.7791\n",
      "Epoch 3/3\n",
      "217/217 [==============================] - 89s 408ms/step - loss: 0.9152 - accuracy: 0.7185 - val_loss: 0.6581 - val_accuracy: 0.7948\n",
      "____________________________________________________________________\n",
      "____________________________________________________________________\n",
      "DenseNet169\n",
      "Found 4338 images belonging to 24 classes.\n",
      "Found 1462 images belonging to 24 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet169_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "51879936/51877672 [==============================] - 0s 0us/step\n",
      "51888128/51877672 [==============================] - 0s 0us/step\n",
      "Epoch 1/3\n",
      "217/217 [==============================] - 105s 439ms/step - loss: 2.0660 - accuracy: 0.4308 - val_loss: 0.8966 - val_accuracy: 0.7291\n",
      "Epoch 2/3\n",
      "217/217 [==============================] - 92s 423ms/step - loss: 0.9806 - accuracy: 0.6992 - val_loss: 0.6618 - val_accuracy: 0.7975\n",
      "Epoch 3/3\n",
      "217/217 [==============================] - 93s 427ms/step - loss: 0.7371 - accuracy: 0.7796 - val_loss: 0.6513 - val_accuracy: 0.7900\n",
      "____________________________________________________________________\n",
      "____________________________________________________________________\n",
      "DenseNet201\n",
      "Found 4338 images belonging to 24 classes.\n",
      "Found 1462 images belonging to 24 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "74842112/74836368 [==============================] - 1s 0us/step\n",
      "74850304/74836368 [==============================] - 1s 0us/step\n",
      "Epoch 1/3\n",
      "217/217 [==============================] - 108s 443ms/step - loss: 1.9057 - accuracy: 0.4636 - val_loss: 0.8870 - val_accuracy: 0.7319\n",
      "Epoch 2/3\n",
      "217/217 [==============================] - 94s 431ms/step - loss: 0.9107 - accuracy: 0.7298 - val_loss: 0.5797 - val_accuracy: 0.8187\n",
      "Epoch 3/3\n",
      "217/217 [==============================] - 93s 430ms/step - loss: 0.6810 - accuracy: 0.7964 - val_loss: 0.5021 - val_accuracy: 0.8482\n",
      "____________________________________________________________________\n",
      "____________________________________________________________________\n",
      "EfficientNetB0\n",
      "Found 4338 images belonging to 24 classes.\n",
      "Found 1462 images belonging to 24 classes.\n",
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
      "16711680/16705208 [==============================] - 0s 0us/step\n",
      "16719872/16705208 [==============================] - 0s 0us/step\n",
      "Epoch 1/3\n",
      "217/217 [==============================] - 91s 388ms/step - loss: 2.1877 - accuracy: 0.3808 - val_loss: 1.2584 - val_accuracy: 0.6320\n",
      "Epoch 2/3\n",
      "217/217 [==============================] - 83s 382ms/step - loss: 1.1494 - accuracy: 0.6531 - val_loss: 0.8236 - val_accuracy: 0.7551\n",
      "Epoch 3/3\n",
      "217/217 [==============================] - 87s 400ms/step - loss: 0.8898 - accuracy: 0.7363 - val_loss: 0.6750 - val_accuracy: 0.7955\n",
      "____________________________________________________________________\n",
      "____________________________________________________________________\n",
      "EfficientNetB1\n",
      "Found 4338 images belonging to 24 classes.\n",
      "Found 1462 images belonging to 24 classes.\n",
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb1_notop.h5\n",
      "27025408/27018416 [==============================] - 1s 0us/step\n",
      "27033600/27018416 [==============================] - 1s 0us/step\n",
      "Epoch 1/3\n",
      "217/217 [==============================] - 98s 413ms/step - loss: 2.0764 - accuracy: 0.4101 - val_loss: 1.1527 - val_accuracy: 0.6539\n",
      "Epoch 2/3\n",
      "217/217 [==============================] - 86s 395ms/step - loss: 1.0987 - accuracy: 0.6752 - val_loss: 0.7802 - val_accuracy: 0.7763\n",
      "Epoch 3/3\n",
      "217/217 [==============================] - 90s 413ms/step - loss: 0.8071 - accuracy: 0.7547 - val_loss: 0.6478 - val_accuracy: 0.8174\n",
      "____________________________________________________________________\n",
      "____________________________________________________________________\n",
      "EfficientNetB2\n",
      "Found 4338 images belonging to 24 classes.\n",
      "Found 1462 images belonging to 24 classes.\n",
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb2_notop.h5\n",
      "31793152/31790344 [==============================] - 1s 0us/step\n",
      "31801344/31790344 [==============================] - 1s 0us/step\n",
      "Epoch 1/3\n",
      "217/217 [==============================] - 100s 420ms/step - loss: 1.8890 - accuracy: 0.4601 - val_loss: 1.0175 - val_accuracy: 0.6867\n",
      "Epoch 2/3\n",
      "217/217 [==============================] - 92s 422ms/step - loss: 1.0347 - accuracy: 0.6837 - val_loss: 0.7581 - val_accuracy: 0.7585\n",
      "Epoch 3/3\n",
      "217/217 [==============================] - 90s 411ms/step - loss: 0.7851 - accuracy: 0.7603 - val_loss: 0.6308 - val_accuracy: 0.7969\n",
      "____________________________________________________________________\n",
      "____________________________________________________________________\n",
      "EfficientNetB3\n",
      "Found 4338 images belonging to 24 classes.\n",
      "Found 1462 images belonging to 24 classes.\n",
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb3_notop.h5\n",
      "43941888/43941136 [==============================] - 1s 0us/step\n",
      "43950080/43941136 [==============================] - 1s 0us/step\n",
      "Epoch 1/3\n",
      "217/217 [==============================] - 104s 429ms/step - loss: 1.8280 - accuracy: 0.4765 - val_loss: 1.0415 - val_accuracy: 0.6990\n",
      "Epoch 2/3\n",
      "217/217 [==============================] - 93s 429ms/step - loss: 0.9672 - accuracy: 0.7059 - val_loss: 0.7331 - val_accuracy: 0.7715\n",
      "Epoch 3/3\n",
      "217/217 [==============================] - 92s 424ms/step - loss: 0.7398 - accuracy: 0.7697 - val_loss: 0.6356 - val_accuracy: 0.7969\n",
      "____________________________________________________________________\n",
      "____________________________________________________________________\n",
      "EfficientNetB4\n",
      "Found 4338 images belonging to 24 classes.\n",
      "Found 1462 images belonging to 24 classes.\n",
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb4_notop.h5\n",
      "71688192/71686520 [==============================] - 1s 0us/step\n",
      "71696384/71686520 [==============================] - 1s 0us/step\n",
      "Epoch 1/3\n",
      "217/217 [==============================] - 110s 443ms/step - loss: 1.8829 - accuracy: 0.4500 - val_loss: 1.1645 - val_accuracy: 0.6512\n",
      "Epoch 2/3\n",
      "217/217 [==============================] - 94s 432ms/step - loss: 1.0760 - accuracy: 0.6697 - val_loss: 0.8834 - val_accuracy: 0.7120\n",
      "Epoch 3/3\n",
      "217/217 [==============================] - 90s 415ms/step - loss: 0.8616 - accuracy: 0.7372 - val_loss: 0.6905 - val_accuracy: 0.7975\n",
      "____________________________________________________________________\n",
      "____________________________________________________________________\n",
      "EfficientNetB5\n",
      "Found 4338 images belonging to 24 classes.\n",
      "Found 1462 images belonging to 24 classes.\n",
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb5_notop.h5\n",
      "115269632/115263384 [==============================] - 3s 0us/step\n",
      "115277824/115263384 [==============================] - 3s 0us/step\n",
      "Epoch 1/3\n",
      "217/217 [==============================] - 117s 462ms/step - loss: 1.8344 - accuracy: 0.4604 - val_loss: 1.0983 - val_accuracy: 0.6703\n",
      "Epoch 2/3\n",
      "217/217 [==============================] - 95s 439ms/step - loss: 1.0916 - accuracy: 0.6598 - val_loss: 0.8672 - val_accuracy: 0.7237\n",
      "Epoch 3/3\n",
      "217/217 [==============================] - 93s 430ms/step - loss: 0.8669 - accuracy: 0.7296 - val_loss: 0.7524 - val_accuracy: 0.7654\n",
      "____________________________________________________________________\n",
      "____________________________________________________________________\n",
      "EfficientNetB6\n",
      "Found 4338 images belonging to 24 classes.\n",
      "Found 1462 images belonging to 24 classes.\n",
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb6_notop.h5\n",
      "165240832/165234480 [==============================] - 4s 0us/step\n",
      "165249024/165234480 [==============================] - 4s 0us/step\n",
      "Epoch 1/3\n",
      "217/217 [==============================] - 122s 474ms/step - loss: 1.9742 - accuracy: 0.4214 - val_loss: 1.4090 - val_accuracy: 0.5397\n",
      "Epoch 2/3\n",
      "217/217 [==============================] - 99s 454ms/step - loss: 1.2277 - accuracy: 0.6224 - val_loss: 1.0627 - val_accuracy: 0.6601\n",
      "Epoch 3/3\n",
      "217/217 [==============================] - 98s 450ms/step - loss: 1.0135 - accuracy: 0.6800 - val_loss: 0.8757 - val_accuracy: 0.7202\n",
      "____________________________________________________________________\n",
      "____________________________________________________________________\n",
      "EfficientNetB7\n",
      "Found 4338 images belonging to 24 classes.\n",
      "Found 1462 images belonging to 24 classes.\n",
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb7_notop.h5\n",
      "258080768/258076736 [==============================] - 7s 0us/step\n",
      "258088960/258076736 [==============================] - 7s 0us/step\n",
      "Epoch 1/3\n",
      "217/217 [==============================] - 131s 501ms/step - loss: 1.8405 - accuracy: 0.4560 - val_loss: 1.1524 - val_accuracy: 0.6539\n",
      "Epoch 2/3\n",
      "217/217 [==============================] - 103s 473ms/step - loss: 1.1261 - accuracy: 0.6528 - val_loss: 0.9621 - val_accuracy: 0.6847\n",
      "Epoch 3/3\n",
      "217/217 [==============================] - 103s 473ms/step - loss: 0.9247 - accuracy: 0.7093 - val_loss: 1.0318 - val_accuracy: 0.6703\n",
      "____________________________________________________________________\n",
      "____________________________________________________________________\n",
      "InceptionResNetV2\n",
      "Found 4338 images belonging to 24 classes.\n",
      "Found 1462 images belonging to 24 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "219062272/219055592 [==============================] - 4s 0us/step\n",
      "219070464/219055592 [==============================] - 4s 0us/step\n",
      "Epoch 1/3\n",
      "217/217 [==============================] - 110s 456ms/step - loss: 2.3033 - accuracy: 0.3430 - val_loss: 1.6235 - val_accuracy: 0.4802\n",
      "Epoch 2/3\n",
      "217/217 [==============================] - 96s 443ms/step - loss: 1.6284 - accuracy: 0.4995 - val_loss: 1.3638 - val_accuracy: 0.5568\n",
      "Epoch 3/3\n",
      "217/217 [==============================] - 97s 447ms/step - loss: 1.4014 - accuracy: 0.5657 - val_loss: 1.2078 - val_accuracy: 0.6026\n",
      "____________________________________________________________________\n",
      "____________________________________________________________________\n",
      "InceptionV3\n",
      "Found 4338 images belonging to 24 classes.\n",
      "Found 1462 images belonging to 24 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - 1s 0us/step\n",
      "87924736/87910968 [==============================] - 1s 0us/step\n",
      "Epoch 1/3\n",
      "217/217 [==============================] - 100s 427ms/step - loss: 2.3237 - accuracy: 0.3555 - val_loss: 1.4942 - val_accuracy: 0.5458\n",
      "Epoch 2/3\n",
      "217/217 [==============================] - 92s 422ms/step - loss: 1.4913 - accuracy: 0.5466 - val_loss: 1.2605 - val_accuracy: 0.5964\n",
      "Epoch 3/3\n",
      "217/217 [==============================] - 90s 416ms/step - loss: 1.2689 - accuracy: 0.6268 - val_loss: 0.9951 - val_accuracy: 0.6984\n",
      "____________________________________________________________________\n",
      "____________________________________________________________________\n",
      "MobileNet\n",
      "Found 4338 images belonging to 24 classes.\n",
      "Found 1462 images belonging to 24 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n",
      "17227776/17225924 [==============================] - 1s 0us/step\n",
      "17235968/17225924 [==============================] - 1s 0us/step\n",
      "Epoch 1/3\n",
      "217/217 [==============================] - 88s 395ms/step - loss: 2.7675 - accuracy: 0.3001 - val_loss: 1.3218 - val_accuracy: 0.5841\n",
      "Epoch 2/3\n",
      "217/217 [==============================] - 94s 435ms/step - loss: 1.4955 - accuracy: 0.5648 - val_loss: 0.9792 - val_accuracy: 0.7086\n",
      "Epoch 3/3\n",
      "217/217 [==============================] - 92s 424ms/step - loss: 1.1738 - accuracy: 0.6604 - val_loss: 0.8214 - val_accuracy: 0.7497\n",
      "____________________________________________________________________\n",
      "____________________________________________________________________\n",
      "MobileNetV2\n",
      "Found 4338 images belonging to 24 classes.\n",
      "Found 1462 images belonging to 24 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9412608/9406464 [==============================] - 0s 0us/step\n",
      "9420800/9406464 [==============================] - 0s 0us/step\n",
      "Epoch 1/3\n",
      "217/217 [==============================] - 99s 438ms/step - loss: 2.1986 - accuracy: 0.3965 - val_loss: 1.0968 - val_accuracy: 0.6772\n",
      "Epoch 2/3\n",
      "217/217 [==============================] - 99s 455ms/step - loss: 1.1796 - accuracy: 0.6491 - val_loss: 0.8154 - val_accuracy: 0.7469\n",
      "Epoch 3/3\n",
      "217/217 [==============================] - 95s 439ms/step - loss: 0.9206 - accuracy: 0.7148 - val_loss: 0.7652 - val_accuracy: 0.7750\n",
      "____________________________________________________________________\n",
      "____________________________________________________________________\n",
      "MobileNetV3Large\n",
      "Found 4338 images belonging to 24 classes.\n",
      "Found 1462 images belonging to 24 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_large_224_1.0_float_no_top.h5\n",
      "17612800/17605208 [==============================] - 0s 0us/step\n",
      "17620992/17605208 [==============================] - 0s 0us/step\n",
      "Epoch 1/3\n",
      "217/217 [==============================] - 99s 433ms/step - loss: 3.2071 - accuracy: 0.0394 - val_loss: 3.1645 - val_accuracy: 0.0465\n",
      "Epoch 2/3\n",
      "217/217 [==============================] - 93s 427ms/step - loss: 3.1793 - accuracy: 0.0498 - val_loss: 3.1483 - val_accuracy: 0.0732\n",
      "Epoch 3/3\n",
      "217/217 [==============================] - 89s 409ms/step - loss: 3.1628 - accuracy: 0.0622 - val_loss: 3.1335 - val_accuracy: 0.1019\n",
      "____________________________________________________________________\n",
      "____________________________________________________________________\n",
      "MobileNetV3Small\n",
      "Found 4338 images belonging to 24 classes.\n",
      "Found 1462 images belonging to 24 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_small_224_1.0_float_no_top.h5\n",
      "6701056/6698480 [==============================] - 0s 0us/step\n",
      "6709248/6698480 [==============================] - 0s 0us/step\n",
      "Epoch 1/3\n",
      "217/217 [==============================] - 95s 412ms/step - loss: 3.1906 - accuracy: 0.0454 - val_loss: 3.1655 - val_accuracy: 0.0800\n",
      "Epoch 2/3\n",
      "217/217 [==============================] - 90s 415ms/step - loss: 3.1711 - accuracy: 0.0521 - val_loss: 3.1569 - val_accuracy: 0.0616\n",
      "Epoch 3/3\n",
      "217/217 [==============================] - 99s 454ms/step - loss: 3.1657 - accuracy: 0.0560 - val_loss: 3.1520 - val_accuracy: 0.0691\n",
      "____________________________________________________________________\n",
      "____________________________________________________________________\n",
      "NASNetLarge\n",
      "Found 4338 images belonging to 24 classes.\n",
      "Found 1462 images belonging to 24 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/nasnet/NASNet-large-no-top.h5\n",
      "343613440/343610240 [==============================] - 7s 0us/step\n",
      "343621632/343610240 [==============================] - 7s 0us/step\n",
      "Epoch 1/3\n",
      "217/217 [==============================] - 229s 961ms/step - loss: 2.0320 - accuracy: 0.4177 - val_loss: 1.4196 - val_accuracy: 0.5513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 15:56:13.024225: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.91GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3\n",
      "217/217 [==============================] - 200s 922ms/step - loss: 1.4506 - accuracy: 0.5726 - val_loss: 1.2660 - val_accuracy: 0.6047\n",
      "Epoch 3/3\n",
      "217/217 [==============================] - 200s 922ms/step - loss: 1.2684 - accuracy: 0.6330 - val_loss: 1.1558 - val_accuracy: 0.6402\n",
      "____________________________________________________________________\n",
      "____________________________________________________________________\n",
      "NASNetMobile\n",
      "Found 4338 images belonging to 24 classes.\n",
      "Found 1462 images belonging to 24 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/nasnet/NASNet-mobile-no-top.h5\n",
      "19996672/19993432 [==============================] - 1s 0us/step\n",
      "20004864/19993432 [==============================] - 1s 0us/step\n",
      "Epoch 1/3\n",
      "217/217 [==============================] - 110s 428ms/step - loss: 2.2277 - accuracy: 0.3757 - val_loss: 1.5872 - val_accuracy: 0.4938\n",
      "Epoch 2/3\n",
      "217/217 [==============================] - 90s 414ms/step - loss: 1.4629 - accuracy: 0.5609 - val_loss: 1.3061 - val_accuracy: 0.5869\n",
      "Epoch 3/3\n",
      "217/217 [==============================] - 95s 439ms/step - loss: 1.1810 - accuracy: 0.6360 - val_loss: 1.0299 - val_accuracy: 0.6683\n",
      "____________________________________________________________________\n",
      "____________________________________________________________________\n",
      "ResNet101\n",
      "Found 4338 images belonging to 24 classes.\n",
      "Found 1462 images belonging to 24 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "171450368/171446536 [==============================] - 4s 0us/step\n",
      "171458560/171446536 [==============================] - 4s 0us/step\n",
      "Epoch 1/3\n",
      "217/217 [==============================] - 100s 430ms/step - loss: 2.4337 - accuracy: 0.4244 - val_loss: 0.9359 - val_accuracy: 0.7196\n",
      "Epoch 2/3\n",
      "217/217 [==============================] - 90s 416ms/step - loss: 1.1860 - accuracy: 0.6676 - val_loss: 0.6591 - val_accuracy: 0.8037\n",
      "Epoch 3/3\n",
      "217/217 [==============================] - 93s 428ms/step - loss: 0.9161 - accuracy: 0.7494 - val_loss: 0.6403 - val_accuracy: 0.8126\n",
      "____________________________________________________________________\n",
      "____________________________________________________________________\n",
      "ResNet101V2\n",
      "Found 4338 images belonging to 24 classes.\n",
      "Found 1462 images belonging to 24 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "171319296/171317808 [==============================] - 2s 0us/step\n",
      "171327488/171317808 [==============================] - 2s 0us/step\n",
      "Epoch 1/3\n",
      "217/217 [==============================] - 103s 431ms/step - loss: 539.3180 - accuracy: 0.0542 - val_loss: 94.8913 - val_accuracy: 0.0622\n",
      "Epoch 2/3\n",
      "217/217 [==============================] - 95s 440ms/step - loss: 26.8383 - accuracy: 0.0493 - val_loss: 5.9989 - val_accuracy: 0.0445\n",
      "Epoch 3/3\n",
      "217/217 [==============================] - 98s 449ms/step - loss: 5.0805 - accuracy: 0.0424 - val_loss: 3.9939 - val_accuracy: 0.0451\n",
      "____________________________________________________________________\n",
      "____________________________________________________________________\n",
      "ResNet152\n",
      "Found 4338 images belonging to 24 classes.\n",
      "Found 1462 images belonging to 24 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet152_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "234700800/234698864 [==============================] - 7s 0us/step\n",
      "234708992/234698864 [==============================] - 7s 0us/step\n",
      "Epoch 1/3\n",
      "217/217 [==============================] - 110s 458ms/step - loss: 2.3621 - accuracy: 0.4401 - val_loss: 0.9178 - val_accuracy: 0.7196\n",
      "Epoch 2/3\n",
      "217/217 [==============================] - 97s 448ms/step - loss: 1.1545 - accuracy: 0.6828 - val_loss: 0.7203 - val_accuracy: 0.7811\n",
      "Epoch 3/3\n",
      "217/217 [==============================] - 98s 452ms/step - loss: 0.9145 - accuracy: 0.7483 - val_loss: 0.5804 - val_accuracy: 0.8283\n",
      "____________________________________________________________________\n",
      "____________________________________________________________________\n",
      "ResNet152V2\n",
      "Found 4338 images belonging to 24 classes.\n",
      "Found 1462 images belonging to 24 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet152v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "234553344/234545216 [==============================] - 6s 0us/step\n",
      "234561536/234545216 [==============================] - 6s 0us/step\n",
      "Epoch 1/3\n",
      "217/217 [==============================] - 109s 454ms/step - loss: 405.5582 - accuracy: 0.0422 - val_loss: 4.0121 - val_accuracy: 0.0342\n",
      "Epoch 2/3\n",
      "217/217 [==============================] - 99s 457ms/step - loss: 4.1348 - accuracy: 0.0376 - val_loss: 4.0544 - val_accuracy: 0.0376\n",
      "Epoch 3/3\n",
      "217/217 [==============================] - 95s 437ms/step - loss: 3.4230 - accuracy: 0.0392 - val_loss: 3.3817 - val_accuracy: 0.0376\n",
      "____________________________________________________________________\n",
      "____________________________________________________________________\n",
      "ResNet50\n",
      "Found 4338 images belonging to 24 classes.\n",
      "Found 1462 images belonging to 24 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94773248/94765736 [==============================] - 2s 0us/step\n",
      "94781440/94765736 [==============================] - 2s 0us/step\n",
      "Epoch 1/3\n",
      "217/217 [==============================] - 97s 430ms/step - loss: 2.4164 - accuracy: 0.4228 - val_loss: 1.1252 - val_accuracy: 0.6778\n",
      "Epoch 2/3\n",
      "217/217 [==============================] - 92s 425ms/step - loss: 1.2327 - accuracy: 0.6547 - val_loss: 0.7295 - val_accuracy: 0.7975\n",
      "Epoch 3/3\n",
      "217/217 [==============================] - 94s 433ms/step - loss: 0.9660 - accuracy: 0.7351 - val_loss: 0.6565 - val_accuracy: 0.8194\n",
      "____________________________________________________________________\n",
      "____________________________________________________________________\n",
      "ResNet50V2\n",
      "Found 4338 images belonging to 24 classes.\n",
      "Found 1462 images belonging to 24 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94674944/94668760 [==============================] - 2s 0us/step\n",
      "94683136/94668760 [==============================] - 2s 0us/step\n",
      "Epoch 1/3\n",
      "217/217 [==============================] - 95s 409ms/step - loss: 289.3665 - accuracy: 0.0556 - val_loss: 36.4457 - val_accuracy: 0.0602\n",
      "Epoch 2/3\n",
      "217/217 [==============================] - 92s 426ms/step - loss: 11.2962 - accuracy: 0.0491 - val_loss: 5.0041 - val_accuracy: 0.0410\n",
      "Epoch 3/3\n",
      "217/217 [==============================] - 88s 405ms/step - loss: 3.9828 - accuracy: 0.0438 - val_loss: 3.8039 - val_accuracy: 0.0424\n",
      "____________________________________________________________________\n",
      "____________________________________________________________________\n",
      "VGG16\n",
      "Found 4338 images belonging to 24 classes.\n",
      "Found 1462 images belonging to 24 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 1s 0us/step\n",
      "58900480/58889256 [==============================] - 1s 0us/step\n",
      "Epoch 1/3\n",
      "217/217 [==============================] - 95s 422ms/step - loss: 16.7430 - accuracy: 0.1452 - val_loss: 8.3303 - val_accuracy: 0.3365\n",
      "Epoch 2/3\n",
      "217/217 [==============================] - 87s 403ms/step - loss: 9.2305 - accuracy: 0.3071 - val_loss: 5.3316 - val_accuracy: 0.4808\n",
      "Epoch 3/3\n",
      "217/217 [==============================] - 87s 399ms/step - loss: 6.3424 - accuracy: 0.4193 - val_loss: 3.9934 - val_accuracy: 0.5622\n",
      "____________________________________________________________________\n",
      "____________________________________________________________________\n",
      "VGG19\n",
      "Found 4338 images belonging to 24 classes.\n",
      "Found 1462 images belonging to 24 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80142336/80134624 [==============================] - 1s 0us/step\n",
      "80150528/80134624 [==============================] - 1s 0us/step\n",
      "Epoch 1/3\n",
      "217/217 [==============================] - 97s 432ms/step - loss: 15.8591 - accuracy: 0.1420 - val_loss: 8.0485 - val_accuracy: 0.3536\n",
      "Epoch 2/3\n",
      "217/217 [==============================] - 98s 451ms/step - loss: 8.5680 - accuracy: 0.3151 - val_loss: 5.2063 - val_accuracy: 0.4959\n",
      "Epoch 3/3\n",
      "217/217 [==============================] - 97s 447ms/step - loss: 6.0798 - accuracy: 0.4149 - val_loss: 4.1658 - val_accuracy: 0.5581\n",
      "____________________________________________________________________\n",
      "____________________________________________________________________\n",
      "Xception\n",
      "Found 4338 images belonging to 24 classes.\n",
      "Found 1462 images belonging to 24 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "83689472/83683744 [==============================] - 3s 0us/step\n",
      "83697664/83683744 [==============================] - 3s 0us/step\n",
      "Epoch 1/3\n",
      "217/217 [==============================] - 101s 452ms/step - loss: 1.7602 - accuracy: 0.4730 - val_loss: 1.2888 - val_accuracy: 0.5951\n",
      "Epoch 2/3\n",
      "217/217 [==============================] - 97s 448ms/step - loss: 1.1339 - accuracy: 0.6524 - val_loss: 0.9729 - val_accuracy: 0.6737\n",
      "Epoch 3/3\n",
      "217/217 [==============================] - 94s 432ms/step - loss: 0.9520 - accuracy: 0.7095 - val_loss: 0.8908 - val_accuracy: 0.7182\n",
      "____________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Loop over each model available in Keras\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "for model_name, model in (model_dictionary.items()):\n",
    "    print('____________________________________________________________________')\n",
    "    print(str(model_name))\n",
    "    traingen, validgen, TS = getGenerators(str(model_name))\n",
    "    \n",
    "        \n",
    "    # load the pre-trained model with global average pooling as the last layer and freeze the model weights\n",
    "    pre_trained_model = model(include_top=False, weights=\"imagenet\", input_shape=TS+(3,))\n",
    "    for layer in pre_trained_model.layers:\n",
    "       layer.trainable = False\n",
    "\n",
    "    # custom modifications on top of pre-trained model\n",
    "    x = tf.keras.layers.Flatten()(pre_trained_model.output)\n",
    "    x = tf.keras.layers.Dense(1024, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)      \n",
    "    x = tf.keras.layers.Dense(24, activation='softmax')(x)           \n",
    "\n",
    "    model = Model(pre_trained_model.input, x) \n",
    "\n",
    "    model.compile(optimizer = RMSprop(learning_rate=0.00001), \n",
    "              loss = 'categorical_crossentropy', \n",
    "              metrics = ['accuracy'])\n",
    "    \n",
    "    history = model.fit(traingen, epochs=3,class_weight=cw, validation_data=validgen)\n",
    "\n",
    "   \n",
    "    model_benchmarks['model_name'].append(model_name)\n",
    "    model_benchmarks['num_model_params'].append(pre_trained_model.count_params())\n",
    "    model_benchmarks['validation_accuracy'].append(history.history['val_accuracy'][-1])\n",
    "    print('____________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8031891",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T17:07:57.060994Z",
     "iopub.status.busy": "2021-12-02T17:07:57.060051Z",
     "iopub.status.idle": "2021-12-02T17:07:57.080831Z",
     "shell.execute_reply": "2021-12-02T17:07:57.081424Z",
     "shell.execute_reply.started": "2021-12-02T11:46:51.212164Z"
    },
    "papermill": {
     "duration": 6.275408,
     "end_time": "2021-12-02T17:07:57.081607",
     "exception": false,
     "start_time": "2021-12-02T17:07:50.806199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>num_model_params</th>\n",
       "      <th>validation_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>MobileNetV3Small</td>\n",
       "      <td>1529968</td>\n",
       "      <td>0.069083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MobileNetV2</td>\n",
       "      <td>2257984</td>\n",
       "      <td>0.774966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MobileNet</td>\n",
       "      <td>3228864</td>\n",
       "      <td>0.749658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EfficientNetB0</td>\n",
       "      <td>4049571</td>\n",
       "      <td>0.795486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MobileNetV3Large</td>\n",
       "      <td>4226432</td>\n",
       "      <td>0.101915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NASNetMobile</td>\n",
       "      <td>4269716</td>\n",
       "      <td>0.668263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EfficientNetB1</td>\n",
       "      <td>6575239</td>\n",
       "      <td>0.817373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DenseNet121</td>\n",
       "      <td>7037504</td>\n",
       "      <td>0.794802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>EfficientNetB2</td>\n",
       "      <td>7768569</td>\n",
       "      <td>0.796854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>EfficientNetB3</td>\n",
       "      <td>10783535</td>\n",
       "      <td>0.796854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DenseNet169</td>\n",
       "      <td>12642880</td>\n",
       "      <td>0.790014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>VGG16</td>\n",
       "      <td>14714688</td>\n",
       "      <td>0.562244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EfficientNetB4</td>\n",
       "      <td>17673823</td>\n",
       "      <td>0.797538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DenseNet201</td>\n",
       "      <td>18321984</td>\n",
       "      <td>0.848153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>VGG19</td>\n",
       "      <td>20024384</td>\n",
       "      <td>0.558140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Xception</td>\n",
       "      <td>20861480</td>\n",
       "      <td>0.718194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>InceptionV3</td>\n",
       "      <td>21802784</td>\n",
       "      <td>0.698358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ResNet50V2</td>\n",
       "      <td>23564800</td>\n",
       "      <td>0.042408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ResNet50</td>\n",
       "      <td>23587712</td>\n",
       "      <td>0.819425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>EfficientNetB5</td>\n",
       "      <td>28513527</td>\n",
       "      <td>0.765390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EfficientNetB6</td>\n",
       "      <td>40960143</td>\n",
       "      <td>0.720246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ResNet101V2</td>\n",
       "      <td>42626560</td>\n",
       "      <td>0.045144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ResNet101</td>\n",
       "      <td>42658176</td>\n",
       "      <td>0.812585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>InceptionResNetV2</td>\n",
       "      <td>54336736</td>\n",
       "      <td>0.602599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ResNet152V2</td>\n",
       "      <td>58331648</td>\n",
       "      <td>0.037620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ResNet152</td>\n",
       "      <td>58370944</td>\n",
       "      <td>0.828317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>EfficientNetB7</td>\n",
       "      <td>64097687</td>\n",
       "      <td>0.670315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NASNetLarge</td>\n",
       "      <td>84916818</td>\n",
       "      <td>0.640219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model_name  num_model_params  validation_accuracy\n",
       "16   MobileNetV3Small           1529968             0.069083\n",
       "14        MobileNetV2           2257984             0.774966\n",
       "13          MobileNet           3228864             0.749658\n",
       "3      EfficientNetB0           4049571             0.795486\n",
       "15   MobileNetV3Large           4226432             0.101915\n",
       "18       NASNetMobile           4269716             0.668263\n",
       "4      EfficientNetB1           6575239             0.817373\n",
       "0         DenseNet121           7037504             0.794802\n",
       "5      EfficientNetB2           7768569             0.796854\n",
       "6      EfficientNetB3          10783535             0.796854\n",
       "1         DenseNet169          12642880             0.790014\n",
       "25              VGG16          14714688             0.562244\n",
       "7      EfficientNetB4          17673823             0.797538\n",
       "2         DenseNet201          18321984             0.848153\n",
       "26              VGG19          20024384             0.558140\n",
       "27           Xception          20861480             0.718194\n",
       "12        InceptionV3          21802784             0.698358\n",
       "24         ResNet50V2          23564800             0.042408\n",
       "23           ResNet50          23587712             0.819425\n",
       "8      EfficientNetB5          28513527             0.765390\n",
       "9      EfficientNetB6          40960143             0.720246\n",
       "20        ResNet101V2          42626560             0.045144\n",
       "19          ResNet101          42658176             0.812585\n",
       "11  InceptionResNetV2          54336736             0.602599\n",
       "22        ResNet152V2          58331648             0.037620\n",
       "21          ResNet152          58370944             0.828317\n",
       "10     EfficientNetB7          64097687             0.670315\n",
       "17        NASNetLarge          84916818             0.640219"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "displaydf = pd.DataFrame(model_benchmarks)\n",
    "displaydf.sort_values('validation_accuracy', inplace=True)\n",
    "displaydf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11055.048922,
   "end_time": "2021-12-02T17:08:06.328671",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-12-02T14:03:51.279749",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
